{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisheries competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we're going to investigate a range of different architectures for the [Kaggle fisheries competition](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/).  The video states that vgg.py and ``vgg_ft()`` from utils.py have been updated to include VGG with batch normalization, but this is not the case.  We've instead created a new file [vgg_bn.py](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/vgg16bn.py) and an additional method ``vgg_ft_bn()`` (which is already in utils.py) which we use in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Setup data dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to copy the dataset to a read-write location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Sansung\n",
      "command.sh\t       __MACOSX     test_stg1\t   vgg16bn.py\n",
      "JsonFile\t       output\t    test_stg1.zip  vgg16bn.pyc\n",
      "jupyterlog.log\t       README.md    train\t   vgg16.py\n",
      "lesson7-Copy1.ipynb    resnet50.py  train.zip\t   vgg16.pyc\n",
      "lesson7.ipynb\t       results\t    utils.py\n",
      "lesson7_jin.ipynb      sample\t    utils.pyc\n",
      "lesson7-jungeun.ipynb  test\t    valid\n"
     ]
    }
   ],
   "source": [
    "# %cd home/ubuntu/Sansung\n",
    "# %cd Sansung\n",
    "\n",
    "!pwd\n",
    "\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Sansung\n",
      "command.sh\t       __MACOSX     test_stg1\t   vgg16bn.py\n",
      "JsonFile\t       output\t    test_stg1.zip  vgg16bn.pyc\n",
      "jupyterlog.log\t       README.md    train\t   vgg16.py\n",
      "lesson7-Copy1.ipynb    resnet50.py  train.zip\t   vgg16.pyc\n",
      "lesson7.ipynb\t       results\t    utils.py\n",
      "lesson7_jin.ipynb      sample\t    utils.pyc\n",
      "lesson7-jungeun.ipynb  test\t    valid\n"
     ]
    }
   ],
   "source": [
    "# % cd /output/data/fish\n",
    "\n",
    "# % cd Sansung\n",
    "\n",
    "!pwd\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo mkdir -p /output/data/fish\n",
    "# %cp /data-fisheries/train.zip /output/data/fish/\n",
    "# !sudo mkdir -p /output/data/fish/test\n",
    "# %cp /data-fisheries/test_stg1.zip /output/data/fish/test\n",
    "\n",
    "# %sudo mkdir -p /output/data/fish/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\n",
      "/home/ubuntu/Sansung/output/data/fish\n",
      "/home/ubuntu\n",
      "/home/ubuntu/Sansung/output/data/fish/test\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "%cd\n",
    "%cd Sansung/output/data/fish\n",
    "zipper = ZipFile('train.zip')\n",
    "zipper.extractall()\n",
    "%cd\n",
    "%cd Sansung/output/data/fish/test\n",
    "zipper = ZipFile('test_stg1.zip')\n",
    "zipper.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot move 'test_stg1' to 'test/test_stg1': Directory not empty\r\n"
     ]
    }
   ],
   "source": [
    "%mv test_stg1 test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We create the validation and sample sets in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\n",
      "/home/ubuntu/Sansung/train\n"
     ]
    }
   ],
   "source": [
    "# %cd /output/data/fish\n",
    "# %cd train\n",
    "# %mkdir ../valid\n",
    "%cd\n",
    "%cd Sansung/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DOL', 'NoF', 'LAG', 'YFT', 'SHARK', 'ALB', 'results', 'BET', 'OTHER']\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 17] File exists: '../valid/DOL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c73421c70dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../valid/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# 여기는 이미 파일 만들어져서 에러 나도 그냥 넘기기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 17] File exists: '../valid/DOL'"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "g = glob('*')\n",
    "print(g)\n",
    "for d in g: os.mkdir('../valid/'+d)\n",
    "# 여기는 이미 파일 만들어져서 에러 나도 그냥 넘기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = glob('*/*.jpg')\n",
    "shuf = np.random.permutation(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(shuf.shape)\n",
    "print(shuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-be4a922d61e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../valid/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# 끝까지 돌아서 인덱스 초과 에러남(이미 한 코드기 때문)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "for i in range(500): os.rename(shuf[i], '../valid/' + shuf[i])\n",
    "# 끝까지 돌아서 인덱스 초과 에러남(이미 한 코드기 때문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../sample’: File exists\n",
      "mkdir: cannot create directory ‘../sample/train’: File exists\n",
      "mkdir: cannot create directory ‘../sample/valid’: File exists\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../sample\n",
    "%mkdir ../sample/train\n",
    "%mkdir ../sample/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 17] File exists: '../sample/train/DOL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5e85e84a2337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../sample/train/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../sample/valid/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 17] File exists: '../sample/train/DOL'"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "g = glob('*')\n",
    "for d in g: \n",
    "    os.mkdir('../sample/train/'+d)\n",
    "    os.mkdir('../sample/valid/'+d)\n",
    "    \n",
    "# File exists 에러는 앞으로 무시해도 됨: 이미 돈 코드기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = glob('*/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(400): copyfile(shuf[i], '../sample/train/' + shuf[i])\n",
    "\n",
    "%cd ../valid\n",
    "\n",
    "g = glob('*/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(200): copyfile(shuf[i], '../sample/valid/' + shuf[i])\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘results’: File exists\n",
      "mkdir: cannot create directory ‘sample/results’: No such file or directory\n",
      "/home/ubuntu\n"
     ]
    }
   ],
   "source": [
    "%mkdir results\n",
    "%mkdir sample/results\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "\u001b[33mWARNING: Skipping theano as it is not installed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo pip uninstall -y theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !sudo pip install theano==0.9\n",
    "import theano\n",
    "theano.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano.sandbox import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%cd ubuntu/Sansung\n",
    "# !ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Sansung\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/Sansung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/output/data/fish/sample/\"\n",
    "path = \"output/data/fish/\"\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3777 images belonging to 8 classes.\n",
      "Found 537 images belonging to 8 classes.\n",
      "Found 3777 images belonging to 8 classes.\n",
      "Found 537 images belonging to 8 classes.\n",
      "Found 3000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's helpful to have just the filenames, without the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_filenames = [f.split('/')[-1] for f in filenames]\n",
    "raw_test_filenames = [f.split('/')[-1] for f in test_filenames]\n",
    "raw_val_filenames = [f.split('/')[-1] for f in val_filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with our usual VGG approach.  We will be using VGG with batch normalization.  We explained how to add batch normalization to VGG in the [imagenet_batchnorm notebook](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/imagenet_batchnorm.ipynb).  VGG with batch normalization is implemented in [vgg_bn.py](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/vgg16bn.py), and there is a version of ``vgg_ft`` (our fine tuning function) with batch norm called ``vgg_ft_bn`` in [utils.py](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/utils.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a simple fine-tuned VGG model to be our starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg16bn import Vgg16BN\n",
    "model = vgg_ft_bn(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trn = get_data(path+'train')\n",
    "val = get_data(path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = get_data(path+'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path+'results/trn.dat', trn)\n",
    "save_array(path+'results/val.dat', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path+'results/test.dat', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = load_array(path+'results/trn.dat')\n",
    "val = load_array(path+'results/val.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_array(path+'results/test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(1e-3),\n",
    "       loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(trn, trn_labels, batch_size=batch_size, nb_epoch=3, validation_data=(val, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(path+'results/ft1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute convolutional output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pre-compute the output of the last convolution layer of VGG, since we're unlikely to need to fine-tune those layers. (All following analysis will be done on just the pre-computed convolutional features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(path+'results/ft1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers,fc_layers = split_at(model, Convolution2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_feat = conv_model.predict(trn)\n",
    "conv_val_feat = conv_model.predict(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_test_feat = conv_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path+'results/conv_val_feat.dat', conv_val_feat)\n",
    "save_array(path+'results/conv_feat.dat', conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path+'results/conv_test_feat.dat', conv_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_feat = load_array(path+'results/conv_feat.dat')\n",
    "conv_val_feat = load_array(path+'results/conv_val_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_test_feat = load_array(path+'results/conv_test_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_val_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our first baseline model - a simple 3-layer FC net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        BatchNormalization(axis=1),\n",
    "        Dropout(p/4),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(8, activation='softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=3, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=7, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/conv_512_6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.evaluate(conv_val_feat, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.load_weights(path+'models/conv_512_6.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are of different sizes, which are likely to represent the boat they came from (since different boats will use different cameras). Perhaps this creates some data leakage that we can take advantage of to get a better Kaggle leaderboard position? To find out, first we create arrays of the file sizes for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [PIL.Image.open(path+'train/'+f).size for f in filenames]\n",
    "id2size = list(set(sizes))\n",
    "size2id = {o:i for i,o in enumerate(id2size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "collections.Counter(sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we one-hot encode them (since we want to treat them as categorical) and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sizes_orig = to_categorical([size2id[o] for o in sizes], len(id2size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_val_sizes = [PIL.Image.open(path+'valid/'+f).size for f in val_filenames]\n",
    "val_sizes = to_categorical([size2id[o] for o in raw_val_sizes], len(id2size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sizes = trn_sizes_orig-trn_sizes_orig.mean(axis=0)/trn_sizes_orig.std(axis=0)\n",
    "val_sizes = val_sizes-trn_sizes_orig.mean(axis=0)/trn_sizes_orig.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this additional \"meta-data\", we create a model with multiple input layers - `sz_inp` will be our input for the size information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(conv_layers[-1].output_shape[1:])\n",
    "sz_inp = Input((len(id2size),))\n",
    "bn_inp = BatchNormalization()(sz_inp)\n",
    "\n",
    "x = MaxPooling2D()(inp)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x = Dropout(p/4)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p/2)(x)\n",
    "x = merge([x,bn_inp], 'concat')\n",
    "x = Dense(8, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compile the model, we have to specify all the input layers in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([inp, sz_inp], x)\n",
    "model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when we train the model, we have to provide all the input layers' data in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([conv_feat, trn_sizes], trn_labels, batch_size=batch_size, nb_epoch=3, \n",
    "             validation_data=([conv_val_feat, val_sizes], val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=8, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model did not show an improvement by using the leakage, other than in the early epochs. This is most likely because the information about what boat the picture came from is readily identified from the image itself, so the meta-data turned out not to add any additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding boxes & multi output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import / view bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kaggle user has created bounding box annotations for each fish in each training set image. You can download them [from here](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/forums/t/25902/complete-bounding-box-annotation). We will see if we can utilize this additional information. First, we'll load in the data, and keep just the largest bounding box for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ujson as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_classes = ['alb', 'bet', 'dol', 'lag', 'other', 'shark', 'yft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations():\n",
    "    annot_urls = {\n",
    "        '5458/bet_labels.json': 'bd20591439b650f44b36b72a98d3ce27',\n",
    "        '5459/shark_labels.json': '94b1b3110ca58ff4788fb659eda7da90',\n",
    "        '5460/dol_labels.json': '91a25d29a29b7e8b8d7a8770355993de',\n",
    "        '5461/yft_labels.json': '9ef63caad8f076457d48a21986d81ddc',\n",
    "        '5462/alb_labels.json': '731c74d347748b5272042f0661dad37c',\n",
    "        '5463/lag_labels.json': '92d75d9218c3333ac31d74125f2b380a'\n",
    "    }\n",
    "    cache_subdir = os.path.abspath(os.path.join(path, 'annos'))\n",
    "    #url_prefix = 'https://kaggle2.blob.core.windows.net/forum-message-attachments/147157/'\n",
    "    #https://storage.googleapis.com/kaggle-forum-message-attachments/147157/5458/bet_labels.json\n",
    "    url_prefix = 'https://storage.googleapis.com/kaggle-forum-message-attachments/147157/'\n",
    "    #http://52.203.141.131:8080/tree/Sansung/JsonFile\n",
    "    \n",
    "    if not os.path.exists(cache_subdir):\n",
    "        os.makedirs(cache_subdir)\n",
    "    \n",
    "    for url_suffix, md5_hash in annot_urls.iteritems():\n",
    "        fname = url_suffix.rsplit('/', 1)[-1]\n",
    "        #get_file(fname, url_prefix , cache_subdir=cache_subdir, md5_hash=md5_hash)\n",
    "        get_file(fname, url_prefix+ url_suffix, cache_subdir=cache_subdir, md5_hash=md5_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_json = {}\n",
    "for c in anno_classes:\n",
    "    if c == 'other': continue # no annotation file for \"other\" class\n",
    "    j = json.load(open('{}annos/{}_labels.json'.format(path, c), 'r'))\n",
    "    for l in j:\n",
    "        if 'annotations' in l.keys() and len(l['annotations'])>0:\n",
    "            bb_json[l['filename'].split('/')[-1]] = sorted(\n",
    "                l['annotations'], key=lambda x: x['height']*x['width'])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_json['img_04908.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2idx = {o:i for i,o in enumerate(raw_filenames)}\n",
    "val_file2idx = {o:i for i,o in enumerate(raw_val_filenames)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any images that have no annotations, we'll create an empty bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_bbox = {'height': 0., 'width': 0., 'x': 0., 'y': 0.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in raw_filenames:\n",
    "    if not f in bb_json.keys(): bb_json[f] = empty_bbox\n",
    "for f in raw_val_filenames:\n",
    "    if not f in bb_json.keys(): bb_json[f] = empty_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert the dictionary into an array, and convert the coordinates to our resized 224x224 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_params = ['height', 'width', 'x', 'y']\n",
    "def convert_bb(bb, size):\n",
    "    bb = [bb[p] for p in bb_params]\n",
    "    conv_x = (224. / size[0])\n",
    "    conv_y = (224. / size[1])\n",
    "    bb[0] = bb[0]*conv_y\n",
    "    bb[1] = bb[1]*conv_x\n",
    "    bb[2] = max(bb[2]*conv_x, 0)\n",
    "    bb[3] = max(bb[3]*conv_y, 0)\n",
    "    return bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_bbox = np.stack([convert_bb(bb_json[f], s) for f,s in zip(raw_filenames, sizes)], \n",
    "                   ).astype(np.float32)\n",
    "val_bbox = np.stack([convert_bb(bb_json[f], s) \n",
    "                   for f,s in zip(raw_val_filenames, raw_val_sizes)]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check our work by drawing one of the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rect(bb, color='red'):\n",
    "    return plt.Rectangle((bb[2], bb[3]), bb[1], bb[0], color=color, fill=False, lw=3)\n",
    "\n",
    "def show_bb(i):\n",
    "    bb = val_bbox[i]\n",
    "    plot(val[i])\n",
    "    plt.gca().add_patch(create_rect(bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bb(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're not allowed (by the kaggle rules) to manually annotate the test set, we'll need to create a model that predicts the locations of the bounding box on each image. To do so, we create a model with multiple outputs: it will predict both the type of fish (the 'class'), and the 4 bounding box coordinates. We prefer this approach to only predicting the bounding box coordinates, since we hope that giving the model more context about what it's looking for will help it with both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(conv_layers[-1].output_shape[1:])\n",
    "x = MaxPooling2D()(inp)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x = Dropout(p/4)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p/2)(x)\n",
    "x_bb = Dense(4, name='bb')(x)\n",
    "x_class = Dense(8, activation='softmax', name='class')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have multiple outputs, we need to provide them to the model constructor in an array, and we also need to say what loss function to use for each. We also weight the bounding box loss function down by 1000x since the scale of the cross-entropy loss and the MSE is very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([inp], [x_bb, x_class])\n",
    "model.compile(Adam(lr=0.001), loss=['mse', 'categorical_crossentropy'], metrics=['accuracy'],\n",
    "             loss_weights=[.001, 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(conv_feat, [trn_bbox, trn_labels], batch_size=batch_size, nb_epoch=3, \n",
    "             validation_data=(conv_val_feat, [val_bbox, val_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(conv_feat, [trn_bbox, trn_labels], batch_size=batch_size, nb_epoch=10, \n",
    "             validation_data=(conv_val_feat, [val_bbox, val_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excitingly, it turned out that the classification model is much improved by giving it this additional task. Let's see how well the bounding box model did by taking a look at its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(conv_val_feat[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bb_pred(i):\n",
    "    bb = val_bbox[i]\n",
    "    bb_pred = pred[0][i]\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plot(val[i])\n",
    "    ax=plt.gca()\n",
    "    ax.add_patch(create_rect(bb_pred, 'yellow'))\n",
    "    ax.add_patch(create_rect(bb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image shows that it can find fish that are tricky for us to see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bb_pred(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(conv_val_feat, [val_bbox, val_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(path+'models/bn_anno.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(path+'models/bn_anno.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we get better results if we use larger images. We'll use 640x360, since it's the same shape as the most common size we saw earlier (1280x720), without being too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = get_data(path+'train', (224,224))\n",
    "val = get_data(path+'valid', (224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image shows that things are much clearer at this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(trn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_data(path+'test', (224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path+'results/trn_640.dat', trn)\n",
    "save_array(path+'results/val_640.dat', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path+'results/test_640.dat', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = load_array(path+'results/trn_640.dat')\n",
    "val = load_array(path+'results/val_640.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our VGG model - we'll need to tell it we're not using the normal 224x224 images, which also means it won't include the fully connected layers (since they don't make sense for non-default sizes). We will also remove the last max pooling layer, since we don't want to throw away information yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg640 = Vgg16BN((224,224)).model\n",
    "vgg640.pop()\n",
    "vgg640.input_shape, vgg640.output_shape\n",
    "vgg640.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pre-compute the output of the convolutional part of VGG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_val_feat = vgg640.predict(val, batch_size=32, verbose=1)\n",
    "conv_trn_feat = vgg640.predict(trn, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path+'results/conv_val_640.dat', conv_val_feat)\n",
    "save_array(path+'results/conv_trn_640.dat', conv_trn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_test_feat = vgg640.predict(test, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path+'results/conv_test_640.dat', conv_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_val_feat = load_array(path+'results/conv_val_640.dat')\n",
    "conv_trn_feat = load_array(path+'results/conv_trn_640.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_test_feat = load_array(path+'results/conv_test_640.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/7, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(conv_val_feat, val_labels, batch_size*2)\n",
    "# help(model)\n",
    "# dir(model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(conv_test_feat, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_array(path+'results/test_640.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_array(path+'results/test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = conv_model.predict(test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = do_clip(preds,0.82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_name = path+'results/subm_bb.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = sorted(batches.class_indices, key=batches.class_indices.get)\n",
    "classes = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'image', raw_test_filenames)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FileLink(subm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results=confusion_matrix(conv_val_feat,val_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "df_cm = pd.DataFrame(results,range(2),range(2))\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "cfreport=classification_report(conv_val_feat, val_labelst)\n",
    "print(\"분류예측 레포트:\\n\", cfreport)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "p2",
   "language": "python",
   "name": "p2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
